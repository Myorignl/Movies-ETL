# Movies-ETL

In this challenge we were tasked with analizing data from different sources, cleaning, restructuring and combining the data through separate iterations until left with usable data. In order to ensure that the original data file was not lost or altered in error through the cleaning process a non destructable file was created and a duplicate file was used in its stead. The jupyter notebook data was then loaded into Postgresql where further queries can be completed.

The files were too large to import via git pull.  
